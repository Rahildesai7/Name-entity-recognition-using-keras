# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fJQVkyMKgzmv3l5AwcGw4CI_CkECdKn5
"""

import keras 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras import layers
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing import sequence
from keras.wrappers.scikit_learn import KerasClassifier
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Input, Dense, Dropout, Embedding, Flatten, LSTM, Bidirectional
from keras.models import Model
from keras.utils import to_categorical
from keras.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

df = pd.read_csv("IMDB Dataset.csv", names=['comment', 'label'], header=0, encoding='utf-8')
df.head()

num_class = len(np.unique(df.label.values))
y = df['label'].values
print(num_class)

plt.hist([len(s) for s in df.comment], bins=50)
plt.show()

max_features = 22000
maxlen = 1600

tokenizer = Tokenizer()
#fits the comments on the comments
tokenizer.fit_on_texts(df.comment.values)
post_seq= tokenizer.texts_to_sequences(df.comment.values)
post_seq_padded= pad_sequences(post_seq, maxlen=maxlen)

X_train, X_test, y_train, y_test = train_test_split(post_seq_padded, y, test_size=0.2, random_state=1)

y_train = np.array(y_train)
y_test = np.array(y_test)

X_train

from sklearn import preprocessing
lb = preprocessing.LabelBinarizer()
y = lb.fit_transform(y_train)
y = to_categorical(y)

input_ = Input(shape=(maxlen,))
model = Embedding(len(tokenizer.word_index)+1, 100, input_length=X_train.shape[1])(input_)
model = Dropout(0.5)(model)
model = Bidirectional(LSTM(64))(model)
model = Dropout(0.5)(model)
out = Dense(2, activation='softmax')(model)
model = Model(input_, out)
model.summary()

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc'])

filepath="weights-simple.hdf5"
checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
history = model.fit(X_train, batch_size=128, y=y, verbose=1, validation_split=0.25, 
          shuffle=True, epochs=3, callbacks=[checkpointer])

df_result = pd.DataFrame({'epochs':history.epoch, 'accuracy': history.history['acc'], 'validation_accuracy': history.history['val_acc']})
g = sns.pointplot(x="epochs", y="accuracy", data=df_result, fit_reg=False)
g = sns.pointplot(x="epochs", y="validation_accuracy", data=df_result, fit_reg=False, color='green')

y_test_ = lb.fit_transform(y_test)
y_test_

#get prediction accuarcy for testing dataset 
predicted = model.predict(X_test)
predicted_best = np.argmax(predicted, axis=1)
print (accuracy_score(predicted_best, y_test_))
predicted=pd.DataFrame(data=predicted)

twt = ['pathetic movie, actor did a bad job']
#vectorizing the tweet by the pre-fitted tokenizer instance
twt = tokenizer.texts_to_sequences(twt)
twt = pad_sequences(twt, maxlen)
print(twt)
sentiment = model.predict(np.array(twt),batch_size=1,verbose = 2)
print(np.argmax(sentiment))
if(np.argmax(sentiment)==0):
  print('negative')
elif(np.argmax(sentiment)==1):
  print('positive')

